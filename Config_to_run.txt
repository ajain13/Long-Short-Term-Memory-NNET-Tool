Note: Data is from the following link - find attached 

The one we used today was sample data not forex.


Configurations to run 

Baseline to compare all is the standard setting given by
(we set adam because all its default values are set including learning rate)

no: layers 3
layer dimensions 1 4 1
optimiser - adam
learning rate - default
momentum - default
error - rmse



Metrics to be recorded - time and rmse
***************************************************************************
Network Structure

***************************************************************************
Variation with number of nodes in default settings

A) Single hidden layer - we will just vary no: of units - simple network

1.  3, 1 6 1, adam, default, default, rmse
2. 3, 1 8 1, adam, default, default, rmse
3. 3, 1 10 1 adam, default, default,rmse

vary units from 5-20 and lets see if we can get a good fit.


Variation with number of layer 

1. 3, 1 best value from A 1 adam, default, default,rmse
2. 4, 1 best value from A 1 adam, default,default,rmse
3. 5, 1 best value from A ... 1 adam, default, default, rmse


****************************************************************************************
Variation with learning rate - SGD and ADAM

Select best setting from A and B and run

eg:

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='mean_squared_error', optimizer=sgd)

1. We will vary the learning rate from the list [1,0.1,0.001,0.002]
keeping all others as default for SGD and ADAM 

2. We have selected SGD and ADAM as they will show significant preformance difference

****************************************************************************************
Effect of Momentum

Select best learning rate for SGD and apply 

1. Momentum from list µt = µ(1−.5×.96)^(t/250) with µ = .99 [for about six t values]

****************************************************************************************

Effect of Weight regularisation


****************************************************************************************

Effect of Dropout



****************************************************************************************